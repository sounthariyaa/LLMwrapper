{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sounthariyaa/LLMwrapper/blob/main/LLM_wrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBLvTBLxP32D",
        "outputId": "b054acfe-193e-40a1-bed4-be24cc2ab178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->openai) (1.3.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install --upgrade tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4SmZWvcsQ9l"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import tiktoken\n",
        "import time\n",
        "from typing import List, Union\n",
        "from pydantic import BaseModel\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkzjm1gYsSI8"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfbZLRJlsWk2"
      },
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "def count_tokens(text, model_name=\"gpt-3.5-turbo-0301\"):\n",
        "    encoding = tiktoken.encoding_for_model(model_name)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def handle_error(error, retry_count, max_retry_attempts, retry_wait_time):\n",
        "    if isinstance(error, openai.error.RateLimitError) and retry_count < max_retry_attempts:\n",
        "        print(f\"Retrying after {retry_wait_time} seconds...\")\n",
        "        time.sleep(retry_wait_time)\n",
        "        return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42rBoxf5o8Ew"
      },
      "outputs": [],
      "source": [
        "class ChatMessage(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "class ChatMessageEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, ChatMessage):\n",
        "            return obj.__dict__  # Convert ChatMessage object to a dictionary\n",
        "        elif isinstance(obj, list) and all(isinstance(item, ChatMessage) for item in obj):\n",
        "            return [item.__dict__ for item in obj]  # Convert list of ChatMessage objects to a list of dictionaries\n",
        "        return super().default(obj)\n",
        "\n",
        "class ChatModelWrapper:\n",
        "    def __init__(self, api_key, use_memory=True, max_completion_token=2000, model_name=\"gpt-3.5-turbo\"):\n",
        "        openai.api_key = api_key\n",
        "        self.memory = []\n",
        "        self.max_completion_token = max_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.use_memory = use_memory\n",
        "\n",
        "    def _manage_memory(self, current_prompt_content, max_tokens):\n",
        "        total_required_token = self.max_completion_token - (count_tokens(current_prompt_content) + max_tokens)\n",
        "        combined_memory_tokens = sum(count_tokens(msg.content) for msg in self.memory)\n",
        "        removed_messages = []\n",
        "\n",
        "        while combined_memory_tokens > total_required_token:\n",
        "            removed_message = self.memory.pop(0)\n",
        "            combined_memory_tokens -= count_tokens(removed_message.content)\n",
        "            removed_messages.append(removed_message)\n",
        "\n",
        "    def _generate_prompt(self, messages, max_tokens):\n",
        "        if self.use_memory:\n",
        "            current_prompt_content = \" \".join(message.content for message in messages)\n",
        "            self._manage_memory(current_prompt_content, max_tokens)\n",
        "            final_messages = self.memory + messages\n",
        "        else:\n",
        "            final_messages = messages\n",
        "        return final_messages\n",
        "\n",
        "    def _chat_completion(self, messages: List[ChatMessage], max_tokens: int = 128, **kwargs) -> openai.ChatCompletion:\n",
        "        retry_count = 0\n",
        "\n",
        "        total_tokens = sum(count_tokens(msg.content) for msg in messages)\n",
        "        if total_tokens + max_tokens > self.max_completion_token:\n",
        "            return \"Error: Total tokens exceed the limit.\"\n",
        "\n",
        "        while retry_count < kwargs.get(\"max_retry_attempts\", 3):\n",
        "            try:\n",
        "                json_string = json.dumps(messages, cls=ChatMessageEncoder)\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=kwargs.get(\"model\", self.model_name),\n",
        "                    messages=json.loads(json_string),  # Load JSON string as a list\n",
        "                    max_tokens=max_tokens,\n",
        "                    **kwargs\n",
        "                )\n",
        "                return response\n",
        "\n",
        "            except openai.error.OpenAIError as e:\n",
        "                if handle_error(e, retry_count, kwargs.get(\"max_retry_attempts\", 3), kwargs.get(\"retry_wait_time\", 60)):\n",
        "                    print(\"Error in _chat_completion: \", e)\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def generate_response(self, messages: List[ChatMessage], max_tokens: int = 128, **kwargs) -> openai.ChatCompletion:\n",
        "        if len(messages) == 0:\n",
        "            return \"Error: No input messages.\"\n",
        "\n",
        "        prompt = self._generate_prompt(messages, max_tokens)\n",
        "\n",
        "        response = self._chat_completion(prompt, **kwargs)\n",
        "\n",
        "        try:\n",
        "            if response and response.choices and len(response.choices) > 0:\n",
        "                if self.use_memory:\n",
        "                    self.memory.extend(messages)\n",
        "                    response_dict = {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": response.choices[0].message.content\n",
        "                    }\n",
        "                    self.memory.append(response_dict)\n",
        "                return response\n",
        "        except:\n",
        "            return response\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_memory_usage(self, use_memory):\n",
        "        self.use_memory = use_memory\n",
        "\n",
        "    def prioritize_messages(self, messages: List[ChatMessage]):\n",
        "        # Sort messages based on timestamp or relevance score\n",
        "        # Ensure that higher-priority messages come first in the list\n",
        "        sorted_messages = sorted(messages, key=lambda msg: msg.timestamp, reverse=True)\n",
        "        return sorted_messages\n",
        "\n",
        "    def split_long_conversation(self, messages: List[ChatMessage], max_tokens_per_chunk):\n",
        "        split_chunks = []\n",
        "        current_chunk = []\n",
        "        current_chunk_tokens = 0\n",
        "\n",
        "        for msg in messages:\n",
        "            msg_tokens = count_tokens(msg.content)\n",
        "            if current_chunk_tokens + msg_tokens <= max_tokens_per_chunk:\n",
        "                current_chunk.append(msg)\n",
        "                current_chunk_tokens += msg_tokens\n",
        "            else:\n",
        "                split_chunks.append(current_chunk)\n",
        "                current_chunk = [msg]\n",
        "                current_chunk_tokens = msg_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            split_chunks.append(current_chunk)\n",
        "\n",
        "        return split_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlIWrPSgtdtV"
      },
      "outputs": [],
      "source": [
        "# Completion Model Wrapper\n",
        "class CompletionModelWrapper:\n",
        "    def __init__(self, api_key, use_memory=True, max_completion_token=3000, model_name=\"text-davinci-003\"):\n",
        "        openai.api_key = api_key\n",
        "        self.memories = []\n",
        "        self.max_completion_token = max_completion_token\n",
        "        self.completion_model_name = model_name\n",
        "        self.use_memory = use_memory\n",
        "        self.max_retry_attempts = 3\n",
        "\n",
        "    def _manage_memory(self, current_prompt, max_tokens):\n",
        "        number_of_token_in_current_prompt = count_tokens(current_prompt, model_name=self.completion_model_name)\n",
        "        total_memory_tokens = sum(count_tokens(memory[\"USER\"], model_name=self.completion_model_name) + count_tokens(memory[\"AI\"], model_name=self.completion_model_name) for memory in self.memories)\n",
        "\n",
        "        while total_memory_tokens > self.max_completion_token - (number_of_token_in_current_prompt + max_tokens):\n",
        "            removed_memory = self.memories.pop(0)\n",
        "            total_memory_tokens -= count_tokens(removed_memory[\"USER\"], model_name=self.completion_model_name) + count_tokens(removed_memory[\"AI\"], model_name=self.completion_model_name)\n",
        "\n",
        "    def _format_conversation(self, current_prompt):\n",
        "        if self.use_memory:\n",
        "            conversation_series = \"\\n\".join([f\"User: {memory['USER']}\\nAI: {memory['AI']}\" for memory in self.memories])\n",
        "            conversation_series += f\"\\nUser: {current_prompt}\\nAI:\"\n",
        "            return conversation_series\n",
        "        else:\n",
        "            return current_prompt\n",
        "\n",
        "    def _completion(self, prompt: str, max_tokens: int = 2000, temperature=1.0, **kwargs) -> openai.Completion:\n",
        "        prompt_with_memory = self._format_conversation(prompt)\n",
        "        retry_count = 0\n",
        "        while retry_count < self.max_retry_attempts:\n",
        "            try:\n",
        "                response = openai.Completion.create(\n",
        "                    model=self.completion_model_name,\n",
        "                    prompt=prompt_with_memory,\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature,  # Control randomness of output\n",
        "                    **kwargs\n",
        "                )\n",
        "                return response\n",
        "            except openai.error.OpenAIError as e:\n",
        "                if handle_error(e.response, retry_count, self.max_retry_attempts, kwargs.get(\"retry_wait_time\", 60)):\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "    def generate_response(self, prompt: str, max_tokens: int = 2000, temperature=1.0, **kwargs) -> openai.Completion:\n",
        "        res = self._completion(prompt, max_tokens, temperature, **kwargs)\n",
        "        if res:\n",
        "            memory = {\n",
        "                \"USER\": prompt,\n",
        "                \"AI\": res.choices[0].text.strip()\n",
        "            }\n",
        "            self.memories.append(memory)\n",
        "            self._manage_memory(prompt, max_tokens)  # Dynamic memory management\n",
        "        return res\n",
        "\n",
        "    def set_model(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_memory_usage(self, use_memory):\n",
        "        self.use_memory = use_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTZZvVWgtfMV"
      },
      "outputs": [],
      "source": [
        "# LLM Wrapper\n",
        "class LLMWrapper:\n",
        "    def __init__(self, api_key, model_type, use_memory=True, max_chat_completion_token=3000, model_name=\"gpt-3.5-turbo\", completion_model_name=\"text-davinci-003\"):\n",
        "        self.api_key = api_key\n",
        "        self.model_type = model_type\n",
        "        self.use_memory = use_memory\n",
        "        self.max_chat_completion_token = max_chat_completion_token\n",
        "        self.model_name = model_name\n",
        "        self.completion_model_name = completion_model_name\n",
        "        self.chat_wrapper = ChatModelWrapper(self.api_key, self.use_memory, self.max_chat_completion_token, self.model_name)\n",
        "        self.completion_wrapper = CompletionModelWrapper(self.api_key, self.use_memory, self.max_chat_completion_token, self.completion_model_name)\n",
        "\n",
        "    def generate_response(self, messages_or_prompt, max_tokens: int = 2000, **kwargs) -> Union[openai.ChatCompletion, openai.Completion, str]:\n",
        "        if isinstance(messages_or_prompt, str):\n",
        "            prompt = messages_or_prompt\n",
        "            messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "        else:\n",
        "            messages = messages_or_prompt\n",
        "            prompt = messages[-1].content\n",
        "\n",
        "        if self.model_type == \"Chat\":\n",
        "            res = self.chat_wrapper.generate_response(messages, max_tokens, **kwargs)\n",
        "            return res.choices[0].message\n",
        "        elif self.model_type == \"Completion\":\n",
        "            res = self.completion_wrapper.generate_response(prompt, max_tokens, **kwargs)\n",
        "            return res.choices[0].text.strip()\n",
        "        else:\n",
        "            return \"Invalid model_type specified.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WBSbwIPz4HF",
        "outputId": "734e2fd3-9f25-4faf-839d-8c81a1c57a1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Response: {\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"Hello! I'm Assistant, an AI trained to help with various tasks. How can I assist you today?\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "llm_wrapper = LLMWrapper(API_KEY, model_type=\"Chat\")\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hello, how are you?\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"I'm doing well, thank you!\")\n",
        "]\n",
        "\n",
        "response = llm_wrapper.generate_response(messages, max_tokens=100)\n",
        "print(\"Generated Response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTSOfrS248GS",
        "outputId": "9eef868f-e806-477b-d48e-358f63b022a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Response: India has achieved many significant milestones in its space program, such as launching the Chandrayaan-1 satellite, sending an orbiter to Mars, launching the first Indian astronaut into space, and many more. India is also making progress in its research and development for the exploration of deep space.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the LLMWrapper with the model type \"Completion\"\n",
        "llm_wrapper = LLMWrapper(API_KEY, model_type=\"Completion\")\n",
        "\n",
        "# Define the prompt for generating completion\n",
        "prompt = \"The space achivements india has had so far\"\n",
        "\n",
        "# Generate a response using the provided prompt\n",
        "response = llm_wrapper.generate_response(prompt)\n",
        "\n",
        "# Print the generated response\n",
        "print(\"Generated Response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2ZS2ZROYdHh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56giiWUBYdHh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}